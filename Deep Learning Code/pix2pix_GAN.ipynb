{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cpRdl4_VrfC"
      },
      "outputs": [],
      "source": [
        "# Importing important libraries\n",
        "!pip install segmentation_models_pytorch\n",
        "import segmentation_models_pytorch as smp\n",
        "import pickle \n",
        "from google.colab import drive\n",
        "import torch, torchvision\n",
        "import numpy as np \n",
        "import random \n",
        "import os\n",
        "from skimage import color\n",
        "from google.colab import drive\n",
        "import torch.cuda as cuda\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bh7Tz06fY0Wx"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, images_names, images_path, split):\n",
        "        'Initialization'\n",
        "        self.images_names = images_names # list of image names : [353397003_1dca2e74c2_138_97443916@N00, 353560364_72da5ae504_163_27027471@N00, ...]\n",
        "        self.images_path = images_path\n",
        "        self.split = split\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.images_names)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "          'Generates one sample of data'\n",
        "          ID = self.images_names[index]\n",
        "          img = Image.open(self.images_path + ID +'.jpg').resize((dimension,dimension))\n",
        "          if self.split == 'Train':\n",
        "            img = torchvision.transforms.RandomHorizontalFlip()(img)\n",
        "\n",
        "          img = img.convert('RGB') # (768,768,3)  \n",
        "          img = np.array(img)  #(768,768,3)\n",
        "          img = color.rgb2lab(img).astype(\"float32\") #(768,768,3)\n",
        "          img = torchvision.transforms.ToTensor()(img) #(3,768,768)  0 -> L  |  1 -> a | 2 -> b\n",
        "\n",
        "          L = torch.unsqueeze(img[0,:,:], 0) #(1,768,768) \n",
        "          ab = img[1:3,:,:] #(2,768,768)\n",
        "\n",
        "          L = (L/50.0) - 1 ## -1 ... 1\n",
        "          ab = ab / 128 ## -1 ... 1\n",
        "\n",
        "          return ab, L\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3GQ1gVggQj0f"
      },
      "outputs": [],
      "source": [
        "# A function that checks if an input image is Black&White or not!  \n",
        "# source: https://stackoverflow.com/questions/23660929/how-to-check-whether-a-jpeg-image-is-color-or-gray-scale-using-only-python-stdli\n",
        "\n",
        "def is_grey_scale(img_path):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    w, h = img.size\n",
        "    for i in range(w):\n",
        "        for j in range(h):\n",
        "            r, g, b = img.getpixel((i,j))\n",
        "            if r != g != b: \n",
        "                return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uXynuCP8PHvl"
      },
      "outputs": [],
      "source": [
        "images_path = \"/content/drive/MyDrive/DS_Total/\"\n",
        "images_names = [\".\".join(image.split(\".\")[:-1]) for image in os.listdir(images_path)]\n",
        "random.shuffle(images_names)\n",
        "\n",
        "######################### JUST RUN THIS BLOCK IN THE FIRST RUN ###############################\n",
        "\n",
        "###### removing Black and White images \n",
        "# print(\"Number of Images Before Removing Black and White Images :  \", len(images_names))\n",
        "# for ID in images_names:\n",
        "#           if is_grey_scale(images_path + ID +'.jpg') == True:\n",
        "#             images_names.remove(ID)\n",
        "# print(\"Number of Images after Removing Black and White Images :  \", len(images_names))\n",
        "\n",
        "##### spliting the data \n",
        "# train_images_names = images_names[:4500]\n",
        "# test_images_names = images_names[4500:]\n",
        "\n",
        "# with open(\"/content/drive/My Drive/val2017/train_2.txt\", \"wb\") as fp:   #Pickling\n",
        "#     pickle.dump(train_images_names, fp)\n",
        "\n",
        "# with open(\"/content/drive/My Drive/val2017/test_2.txt\", \"wb\") as fp:   #Pickling\n",
        "#     pickle.dump(test_images_names, fp)\n",
        "\n",
        "###############################################################################################\n",
        "\n",
        "\n",
        "with open(\"/content/drive/My Drive/val2017/train_2.txt\", \"rb\") as fp:   # Unpickling\n",
        "    train_images_names = pickle.load(fp)\n",
        "\n",
        "with open(\"/content/drive/My Drive/val2017/test_2.txt\", \"rb\") as fp:   # Unpickling\n",
        "    test_images_names = pickle.load(fp)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of Images Before Removing Black and White Images :   5085\n",
        "# Number of Images after Removing Black and White Images :   4958"
      ],
      "metadata": {
        "id": "jFNExzEoWpJA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfokQCe-HEX0",
        "outputId": "7e3861a3-ac54-4968-ddd8-cc97969d773b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lentgh of Train Images : 4500\n",
            "Lentgh of Test Images : 458\n",
            "\n",
            "Number of train batches : 1500\n",
            "Number of test batches : 28\n"
          ]
        }
      ],
      "source": [
        "epochs = 0\n",
        "val_best_loss = np.inf\n",
        "dimension = 768\n",
        "alpha = 10\n",
        "disc_iteration =  1\n",
        "\n",
        "batch_size_train = 3\n",
        "batch_size_test = 16\n",
        "\n",
        "params_train = {\n",
        "        'batch_size' : batch_size_train ,\n",
        "        'shuffle': True , \n",
        "        'num_workers': 0\n",
        "}        \n",
        "\n",
        "params_test = {\n",
        "        'batch_size' : batch_size_test ,\n",
        "        'shuffle': False , \n",
        "        'num_workers': 0\n",
        "}\n",
        "\n",
        "train_data = Dataset(train_images_names, images_path, 'Train')\n",
        "train_data_generator = torch.utils.data.DataLoader(train_data, **params_train, drop_last=True)\n",
        "\n",
        "test_data = Dataset(test_images_names, images_path, 'Test')\n",
        "test_data_generator = torch.utils.data.DataLoader(test_data, **params_test,  drop_last=False)\n",
        "\n",
        "number_of_train_batches = len(train_images_names) // batch_size_train \n",
        "number_of_test_batches = len(test_images_names) // batch_size_test \n",
        "\n",
        "print(\"\\nLentgh of Train Images : {}\".format(len(train_images_names)))\n",
        "print(\"Lentgh of Test Images : {}\\n\".format(len(test_images_names)))\n",
        "print(\"Number of train batches :\",number_of_train_batches)\n",
        "print(\"Number of test batches :\",number_of_test_batches)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiSST_1o61Wf",
        "outputId": "a75f286c-3682-461e-bdd7-1f1705cc5c10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape of Discriminator :  torch.Size([1, 1, 94, 94])\n"
          ]
        }
      ],
      "source": [
        "class patch_GAN(torch.nn.Module): # 3 * 768 * 768\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1=torch.nn.Conv2d(3, 4, kernel_size = 4, stride = 2, padding = 1 ) # size =  torch.Size([batch_size, 4, 384, 384])\n",
        "    self.bn1 = torch.nn.BatchNorm2d(4)\n",
        "    self.relu=torch.nn.ReLU() \n",
        "\n",
        "    self.conv2=torch.nn.Conv2d(4, 4, kernel_size = 4, stride = 2, padding = 1 ) # size =  torch.Size([batch_size, 4, 192, 192])\n",
        "    self.bn2 = torch.nn.BatchNorm2d(4)\n",
        "\n",
        "    self.conv3=torch.nn.Conv2d(4, 4, kernel_size = 4, stride = 2, padding = 1 ) # size =  torch.Size([batch_size, 4, 96, 96])\n",
        "    self.bn3 = torch.nn.BatchNorm2d(4)\n",
        "\n",
        "    self.conv4=torch.nn.Conv2d(4, 4, kernel_size = 4, stride = 1, padding = 1 ) # size =  torch.Size([batch_size, 4, 95, 95])\n",
        "    self.bn4 = torch.nn.BatchNorm2d(4)\n",
        "\n",
        "    self.conv5=torch.nn.Conv2d(4, 1, kernel_size = 4, stride = 1, padding = 1 ) # size =  torch.Size([batch_size, 4, 94, 94])\n",
        "    self.sigmoid=torch.nn.Sigmoid() \n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    out=self.conv1(x)\n",
        "    out=self.bn1(out)\n",
        "    out=self.relu(out)\n",
        "\n",
        "    out=self.conv2(out)\n",
        "    out=self.bn2(out)\n",
        "    out=self.relu(out)\n",
        "\n",
        "    out=self.conv3(out)\n",
        "    out=self.bn3(out)\n",
        "    out=self.relu(out)\n",
        "\n",
        "    out=self.conv4(out)\n",
        "    out=self.bn4(out)\n",
        "    out=self.relu(out)\n",
        "\n",
        "    out=self.conv5(out)\n",
        "    out=self.sigmoid(out) # size =  torch.Size([batch_size, 1, 94, 94])\n",
        "\n",
        "    return out \n",
        "\n",
        "discriminator = patch_GAN()\n",
        "x  = torch.randn(1, 3, dimension, dimension)  # A random image\n",
        "output = discriminator(x)\n",
        "print(\"Output shape of Discriminator : \", output.shape)\n",
        "\n",
        "discriminator_output_dimension = output.shape[2] # 94\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aenOu03_gp31"
      },
      "outputs": [],
      "source": [
        "# https://github.com/qubvel/segmentation_models.pytorch\n",
        "generator = smp.Unet(\n",
        "    encoder_name=\"efficientnet-b5\", # choose encoder, e.g. xception , mobilenet_v2 or efficientnet-b7\n",
        "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
        "    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
        "    classes=2\n",
        ")\n",
        "discriminator = patch_GAN()\n",
        "\n",
        "generator.float()\n",
        "discriminator.float()\n",
        "\n",
        "\n",
        "if cuda.is_available():\n",
        "         generator = generator.cuda()\n",
        "         discriminator = discriminator.cuda()\n",
        "\n",
        "BCE_loss = torch.nn.BCELoss()\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.001)\n",
        "\n",
        "checkpoint=torch.load(\"/content/drive/My Drive/val2017/Generator_2.pth\")\n",
        "generator.load_state_dict(checkpoint)\n",
        "\n",
        "checkpoint=torch.load(\"/content/drive/My Drive/val2017/Discriminator_2.pth\")\n",
        "discriminator.load_state_dict(checkpoint)\n",
        "\n",
        "checkpoint = torch.load(\"/content/drive/My Drive/val2017/Optimizer_G_2.pth\")\n",
        "optimizer_G.load_state_dict(checkpoint)\n",
        "\n",
        "checkpoint = torch.load(\"/content/drive/My Drive/val2017/Optimizer_D_2.pth\")\n",
        "optimizer_D.load_state_dict(checkpoint)\n",
        "\n",
        "with open(\"/content/drive/My Drive/val2017/Epoch_2.txt\", \"r\") as f:   \n",
        "   epochs = int(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zw_Q6Rm7alsB"
      },
      "outputs": [],
      "source": [
        "########################################### TRAINING ##########################################\n",
        "generator = generator.train()\n",
        "for epoch in range(epochs+1, epochs + 100):\n",
        "\n",
        "  disc_losses = []\n",
        "  gen_losses = []\n",
        "  for colored_images,  BW_images in train_data_generator:\n",
        "    \n",
        "     for k in range(disc_iteration):\n",
        "        #  Colored_images(Original image): tensor (batch_size, 2, 768, 768)  |  BW_images : Corrosponds to Colored_images  , tensor (batch_size, 1, 768, 768)\n",
        "        # In fact-> colored_images contain ab-channels for the images (Real version) ,\n",
        "        #          BW_images contain L-channel for the images (Real version)\n",
        "        if cuda.is_available():\n",
        "            BW_images = BW_images.cuda()\n",
        "            colored_images = colored_images.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "           Generated_colored_image = generator(BW_images).detach() # tensor (batch_size, 2, 768, 768)\n",
        "           # In fact-> Generated_colored_image contain the predicted values for ab-channels (Fake version)\n",
        "\n",
        "\n",
        "        BW_and_Generated_images = torch.cat((BW_images, Generated_colored_image), dim = 1) # tensor (batch_size, 3, 768, 768)\n",
        "        BW_and_colored_images = torch.cat((BW_images, colored_images), dim = 1) # tensor (batch_size, 3, 768, 768)\n",
        "        \n",
        "        label_0_for_BW_and_Generated_images = torch.zeros(batch_size_train, 1, discriminator_output_dimension, discriminator_output_dimension)\n",
        "        label_1_for_BW_and_colored_images = torch.ones(batch_size_train, 1, discriminator_output_dimension, discriminator_output_dimension)\n",
        "\n",
        "        x = torch.cat((BW_and_Generated_images, BW_and_colored_images), dim = 0) # tensor (2*batch_size, 3, 768, 768)\n",
        "        y = torch.cat((label_0_for_BW_and_Generated_images,label_1_for_BW_and_colored_images), dim = 0) # tensor (2*batch_size, 1, 94, 94)\n",
        "\n",
        "        c = list(zip(x, y))\n",
        "        random.shuffle(c)\n",
        "        x, y = zip(*c) \n",
        "\n",
        "        \n",
        "        x = torch.stack(list(x), dim=0) # tensor (2*batch_size, 3, 768, 768)\n",
        "        y = torch.stack(list(y), dim=0) # tensor (2*batch_size, 1, 94, 94)\n",
        "        \n",
        "        if cuda.is_available():\n",
        "            y = y.cuda()\n",
        "            x = x.cuda()\n",
        "\n",
        "        dis = discriminator(x.float()) # tensor (2*batch_size, 1, 94, 94) \n",
        "        dis = dis.view(-1, 1)  # tensor (2*batch_size*94*94 , 1) \n",
        "        y = y.view(-1, 1).float()  # tensor (2*batch_size*94*94 , 1) \n",
        "\n",
        "        disc_loss = BCE_loss(dis, target = y) * 0.5\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        disc_loss.backward()\n",
        "        optimizer_D.step()\n",
        "        disc_losses.append(disc_loss.item())\n",
        "\n",
        "        \n",
        "\n",
        "     ################################################################################################\n",
        "     if cuda.is_available():\n",
        "            BW_images = BW_images.cuda()\n",
        "\n",
        "     Generated_colored_image = generator(BW_images) # tensor (batch_size, 2, 768, 768)\n",
        "\n",
        "     BW_and_Generated_images = torch.cat((BW_images, Generated_colored_image), dim = 1) # tensor (batch_size, 3, 768, 768)\n",
        "\n",
        "\n",
        "     if cuda.is_available():\n",
        "            BW_and_Generated_images = BW_and_Generated_images.cuda()\n",
        "            colored_images = colored_images.cuda()\n",
        "\n",
        "     dis = discriminator(BW_and_Generated_images.float()) # tensor (batch_size, 1, 94, 94) \n",
        "     dis = dis.view(-1, 1)  # tensor (batch_size*94*94 , 1) \n",
        "     y = torch.ones(batch_size_train, 1, discriminator_output_dimension, discriminator_output_dimension)\n",
        "     y = y.view(-1, 1).float()  # tensor (batch_size*94*94 , 1) \n",
        "     if cuda.is_available():\n",
        "            y = y.cuda()\n",
        "\n",
        "     pixel_distance_loss_tensor =  torch.subtract(Generated_colored_image, colored_images) # tensor (batch_size, 3, 768, 768)\n",
        "     pixel_distance_loss_tensor =  pixel_distance_loss_tensor.view(pixel_distance_loss_tensor.shape[0], -1) # tensor (batch_size, 3*768*768)\n",
        "     pixel_distance_loss_tensor_norm = torch.norm(pixel_distance_loss_tensor, p=1, dim=1).reshape(-1,1) # tensor (batch_size, 1)\n",
        "     pixel_distance_loss = torch.mean(pixel_distance_loss_tensor_norm,0) # a number\n",
        "\n",
        "     gen_loss =   BCE_loss(dis, target = y) + alpha * pixel_distance_loss  # a number\n",
        "\n",
        "     optimizer_G.zero_grad()   \n",
        "     gen_loss.backward()\n",
        "     optimizer_G.step()\n",
        "\n",
        "     gen_losses.append(gen_loss.item())\n",
        "\n",
        "\n",
        "\n",
        "     ################################################################################################\n",
        "\n",
        "  print(\"Iteration : {} -> Train , D_LOSS : {}\".format(epoch, np.mean(disc_losses)))\n",
        "  print(\"Iteration : {} -> Train , G_LOSS : {}\".format(epoch, np.mean(gen_losses)))\n",
        "  \n",
        "  if epoch%3==0:\n",
        "      torch.save(generator.state_dict(),\"/content/drive/My Drive/val2017/Generator_\"+str(epoch)+\"_2.pth\")\n",
        "      torch.save(discriminator.state_dict(),\"/content/drive/My Drive/val2017/Discriminator_\"+str(epoch)+\"_2.pth\")\n",
        "      torch.save(optimizer_G.state_dict(),\"/content/drive/My Drive/val2017/Optimizer_G_\"+str(epoch)+\"_2.pth\")\n",
        "      torch.save(optimizer_D.state_dict(),\"/content/drive/My Drive/val2017/Optimizer_D_\"+str(epoch)+\"_2.pth\")\n",
        "      print(\"[SAVED !]\")\n",
        "\n",
        "\n",
        "  torch.save(generator.state_dict(),\"/content/drive/My Drive/val2017/Generator_2.pth\")\n",
        "  torch.save(discriminator.state_dict(),\"/content/drive/My Drive/val2017/Discriminator_2.pth\")\n",
        "  torch.save(optimizer_G.state_dict(),\"/content/drive/My Drive/val2017/Optimizer_G_2.pth\")\n",
        "  torch.save(optimizer_D.state_dict(),\"/content/drive/My Drive/val2017/Optimizer_D_2.pth\")\n",
        "\n",
        "\n",
        "  with open(\"/content/drive/My Drive/val2017/Epoch_2.txt\", \"w+\") as f :   \n",
        "      f.write(str(epoch))\n",
        "\n",
        "  with open(\"/content/drive/My Drive/val2017/Results_2.txt\", \"a+\") as f:   \n",
        "     f.write(\"\\n Iteration : {} -> Train D_LOSS : {}\".format(epoch, np.mean(disc_losses)))\n",
        "     f.write(\"\\n Iteration : {} -> Train G_LOSS : {}\".format(epoch, np.mean(gen_losses)))\n",
        "\n",
        "\n",
        "  print(\"______________________________________________\")\n",
        "\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}